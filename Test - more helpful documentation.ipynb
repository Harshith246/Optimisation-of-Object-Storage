{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Big Data Optimisation in Object storage using different file formats", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Data access in efficient way is one of the prominent factors for having a good performance data processing pipeline . Identifying the layout of data values in the filesystem has basic impacts on the overall performance of data access. We are going to show the insights on how data layout affects the performance of data access. We will differentiate the difference between different columnar file formats and row file formats, explain how to use them efficiently to store data values. Then we would demonstrate the best practice on how to store datasets which would include the concepts partitioning and bucketing.\n\nThe main objective is to find the cost reducing and efficient way to reduce the capital expense and increase the operating efficiency while working with the Big data in cloud object storages(S3 , IBM COS)\n\nThis notebook will be a guide to Optimising Bigdata queries using denormalization and different file formats(CSV,Json and Parquet etc). It is expected that you will run this notebook on a local jupyter environment or from DSX . \n\nWe would be using the datasets from movielens(https://grouplens.org/datasets/movielens/20m/) as a data source for analysis. \n\nThe main tests we are going to do:\n\n- Test 1: Time how long it takes to read in two csv datasets, join them and then do a count on the resulting (denormalised) dataset.  \n   - Note: The denormalised dataset from step 1 is saved to cloud object storage as a CSV, Partioned Parquet and JSON and used in subsequent steps\n- Test 2: Time how long it takes to read the denormalised csv dataset from COS.  This should be quicker than step 1 because CSV is very naive and simple,reading directly from it will be very quick;select, modify and manipulate data would add much time cost to your call\n- Test 3: Time how long it takes to read the denormalised json dataset from COS.  This should be less quicker than step 2 and step 1 because CSV file size is less than JSON and less data needs to be read from the disk .\n- Test 4: Time how long it takes to read the denormalised parquet dataset from COS.  This should be much quicker than step 1,2 and 3 because it provides columnar compression, which saves storage space and allows for reading individual columns instead of entire files.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Load the main data sets\n\nIn this section we load two datasets, ratings.csv and movies.csv", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 13, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(userId='1', movieId='2', rating='3.5', timestamp='1112486027'),\n Row(userId='1', movieId='29', rating='3.5', timestamp='1112484676'),\n Row(userId='1', movieId='32', rating='3.5', timestamp='1112484819'),\n Row(userId='1', movieId='47', rating='3.5', timestamp='1112484727'),\n Row(userId='1', movieId='50', rating='3.5', timestamp='1112484580')]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# Load the ratings.csv file to Object storage as a Spark dataset from https://grouplens.org/datasets/movielens/20m/ \nimport ibmos2spark\n\ncredentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'api_key': 'fYpsl9yOVZc4nCZkGsO8c4EVFwIA9chGtafX6bva8vRn',\n    'service_id': 'iam-ServiceId-8312f186-2ed5-4d25-a8b9-744a2f5b0434',\n    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'}\n\nconfiguration_name = 'os_3269765585d54624a3f9e9ca7b7a831e_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_data_1 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('ratings.csv', 'optimisationofbigdataperformanc264b8c876f09466290ceff9606e7dd66'))\ndf_data_1.take(5)"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+-------+------+----------+\n|userId|movieId|rating| timestamp|\n+------+-------+------+----------+\n|     1|      2|   3.5|1112486027|\n|     1|     29|   3.5|1112484676|\n|     1|     32|   3.5|1112484819|\n|     1|     47|   3.5|1112484727|\n|     1|     50|   3.5|1112484580|\n|     1|    112|   3.5|1094785740|\n|     1|    151|   4.0|1094785734|\n|     1|    223|   4.0|1112485573|\n|     1|    253|   4.0|1112484940|\n|     1|    260|   4.0|1112484826|\n|     1|    293|   4.0|1112484703|\n|     1|    296|   4.0|1112484767|\n|     1|    318|   4.0|1112484798|\n|     1|    337|   3.5|1094785709|\n|     1|    367|   3.5|1112485980|\n|     1|    541|   4.0|1112484603|\n|     1|    589|   3.5|1112485557|\n|     1|    593|   3.5|1112484661|\n|     1|    653|   3.0|1094785691|\n|     1|    919|   3.5|1094785621|\n+------+-------+------+----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "df_data_1.show()"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(movieId='1', title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy'),\n Row(movieId='2', title='Jumanji (1995)', genres='Adventure|Children|Fantasy'),\n Row(movieId='3', title='Grumpier Old Men (1995)', genres='Comedy|Romance'),\n Row(movieId='4', title='Waiting to Exhale (1995)', genres='Comedy|Drama|Romance'),\n Row(movieId='5', title='Father of the Bride Part II (1995)', genres='Comedy')]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# load movies.csv file to Object storage as a Spark dataset from https://grouplens.org/datasets/movielens/20m/ \n\ndf_data_3 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('movies.csv', 'optimisationofbigdataperformanc264b8c876f09466290ceff9606e7dd66'))\ndf_data_3.take(5)\n"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+--------------------+--------------------+\n|movieId|               title|              genres|\n+-------+--------------------+--------------------+\n|      1|    Toy Story (1995)|Adventure|Animati...|\n|      2|      Jumanji (1995)|Adventure|Childre...|\n|      3|Grumpier Old Men ...|      Comedy|Romance|\n|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n|      5|Father of the Bri...|              Comedy|\n|      6|         Heat (1995)|Action|Crime|Thri...|\n|      7|      Sabrina (1995)|      Comedy|Romance|\n|      8| Tom and Huck (1995)|  Adventure|Children|\n|      9| Sudden Death (1995)|              Action|\n|     10|    GoldenEye (1995)|Action|Adventure|...|\n|     11|American Presiden...|Comedy|Drama|Romance|\n|     12|Dracula: Dead and...|       Comedy|Horror|\n|     13|        Balto (1995)|Adventure|Animati...|\n|     14|        Nixon (1995)|               Drama|\n|     15|Cutthroat Island ...|Action|Adventure|...|\n|     16|       Casino (1995)|         Crime|Drama|\n|     17|Sense and Sensibi...|       Drama|Romance|\n|     18|   Four Rooms (1995)|              Comedy|\n|     19|Ace Ventura: When...|              Comedy|\n|     20|  Money Train (1995)|Action|Comedy|Cri...|\n+-------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "df_data_3.show()"
        }, 
        {
            "source": "### Test 1 - Join datasets at query time\n\nIn this section we want to see how long it takes to load two separate datasets and join them", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Denormalize the two datasets df_data_1 and df_data_3 using a join ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_data_4 = df_data_1.join(df_data_3,\"movieId\")"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+------+------+----------+--------------------+--------------------+\n|movieId|userId|rating| timestamp|               title|              genres|\n+-------+------+------+----------+--------------------+--------------------+\n|      2|     1|   3.5|1112486027|      Jumanji (1995)|Adventure|Childre...|\n|     29|     1|   3.5|1112484676|City of Lost Chil...|Adventure|Drama|F...|\n|     32|     1|   3.5|1112484819|Twelve Monkeys (a...|Mystery|Sci-Fi|Th...|\n|     47|     1|   3.5|1112484727|Seven (a.k.a. Se7...|    Mystery|Thriller|\n|     50|     1|   3.5|1112484580|Usual Suspects, T...|Crime|Mystery|Thr...|\n|    112|     1|   3.5|1094785740|Rumble in the Bro...|Action|Adventure|...|\n|    151|     1|   4.0|1094785734|      Rob Roy (1995)|Action|Drama|Roma...|\n|    223|     1|   4.0|1112485573|       Clerks (1994)|              Comedy|\n|    253|     1|   4.0|1112484940|Interview with th...|        Drama|Horror|\n|    260|     1|   4.0|1112484826|Star Wars: Episod...|Action|Adventure|...|\n|    293|     1|   4.0|1112484703|L\u00e9on: The Profess...|Action|Crime|Dram...|\n|    296|     1|   4.0|1112484767| Pulp Fiction (1994)|Comedy|Crime|Dram...|\n|    318|     1|   4.0|1112484798|Shawshank Redempt...|         Crime|Drama|\n|    337|     1|   3.5|1094785709|What's Eating Gil...|               Drama|\n|    367|     1|   3.5|1112485980|    Mask, The (1994)|Action|Comedy|Cri...|\n|    541|     1|   4.0|1112484603| Blade Runner (1982)|Action|Sci-Fi|Thr...|\n|    589|     1|   3.5|1112485557|Terminator 2: Jud...|       Action|Sci-Fi|\n|    593|     1|   3.5|1112484661|Silence of the La...|Crime|Horror|Thri...|\n|    653|     1|   3.0|1094785691|  Dragonheart (1996)|Action|Adventure|...|\n|    919|     1|   3.5|1094785621|Wizard of Oz, The...|Adventure|Childre...|\n+-------+------+------+----------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "df_data_4.show()"
        }, 
        {
            "source": "Analysing the time how long it takes to do a count of denormalised dataset,joined at query time  which took 34.3 Seconds .", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 19, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "20000263"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_4.count()"
        }, 
        {
            "source": "Analysing the time how long it takes to do a count of records from data set joined at the time of querying grouped by rating attribute , it took 35.69", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+-------+\n|rating|  count|\n+------+-------+\n|   1.0| 680732|\n|   4.5|1534824|\n|   2.5| 883398|\n|   3.5|2200156|\n|   5.0|2898660|\n|   0.5| 239125|\n|   4.0|5561926|\n|   1.5| 279252|\n|   2.0|1430997|\n|   3.0|4291193|\n+------+-------+\n\n"
                }
            ], 
            "source": "df_data_4.groupBy(\"rating\").count().show()"
        }, 
        {
            "source": "### Test 2 :Converting and Reading the denormalised CSV dataset\n\nIn this section we want to see how long it takes to convert spark dataframe into CSV and analyse the query time", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Converting the denormalised spark dataframe into CSV file with header \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "ename": "AnalysisException", 
                    "evalue": "'path file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/Sample2csv already exists.;'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o137.csv.\n: org.apache.spark.sql.AnalysisException: path file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/Sample2csv already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:484)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:520)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:579)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n", 
                        "\nDuring handling of the above exception, another exception occurred:\n", 
                        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-21-c7c76064ec85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_data_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sample2csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# PLease make sure to involve the option header = 'true' to get the header involved in CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat)\u001b[0m\n\u001b[1;32m    709\u001b[0m                        \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnullValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescapeQuotes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mescapeQuotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoteAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquoteAll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                        dateFormat=dateFormat, timestampFormat=timestampFormat)\n\u001b[0;32m--> 711\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/Sample2csv already exists.;'"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "df_data_4.write.csv('Sample2csv',header = 'true')\n# PLease make sure to involve the option header = 'true' to get the header involved in CSV file "
        }, 
        {
            "source": "Making the csv file readable by Spark using csvFile", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Defining the CSV File as csvFile.\ncsvFile = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"mode\", \"FAILFAST\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/Sample2csv\")"
        }, 
        {
            "source": "Analysing the time how long it takes to do a count of records in convertedCSV file, which took 6.5 Seconds .", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvFile.count()"
        }, 
        {
            "source": "Analysing the time how long it takes to do a count of records from csv file grouped by rating attribute , it took 7.12 sec ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "countsByRating = csvFile.groupBy(\"rating\").count().show()"
        }, 
        {
            "source": "### Test 3: Reading the denormalised Json dataset\n\nIn this section we want to see how long it takes from CSV to Json and analyse the query time", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Converting the CSV file to Json file", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvFile.write.format(\"json\").mode(\"overwrite\")\\\n  .save(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/Sample.json\")"
        }, 
        {
            "source": "Analysing the time how long it takes to do a count of records in converted Json file, which took 23.79 Seconds", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 22, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "20000263"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/Sample.json\").count()"
        }, 
        {
            "source": "Analysing the time how long it takes to do a count of records from josn file grouped by rating attribute , it took 16.12 sec ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+-------+\n|rating|  count|\n+------+-------+\n|   3.5|2200156|\n|   4.5|1534824|\n|   2.5| 883398|\n|   1.0| 680732|\n|   4.0|5561926|\n|   0.5| 239125|\n|   3.0|4291193|\n|   2.0|1430997|\n|   1.5| 279252|\n|   5.0|2898660|\n+------+-------+\n\n"
                }
            ], 
            "source": "spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/Sample.json\").groupBy(\"rating\").count().show()"
        }, 
        {
            "source": "### Test 4: Reading the denormalised parquet dataset\n\nIn this section we want to see how long it takes to convert csv to parquet and analyse the query time", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Conversion of CSV file to parquet file", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvFile.write.mode(\"overwrite\").partitionBy(\"rating\")\\\n  .save(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/partitioned-files.parquet\")"
        }, 
        {
            "source": "Analysing the time how long it takes to do a count of records in converted parquet file, which took 1.81 Seconds", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 24, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "20000263"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "spark.read.format(\"parquet\")\\\n  .load(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/partitioned-files.parquet\").count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "spark.read.format(\"parquet\")\\\n  .load(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/partitioned-files.parquet\").show()"
        }, 
        {
            "source": "Analysing the time how long it takes to do a count of records in converted parquet file, which took 3.92 Seconds", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+-------+\n|rating|  count|\n+------+-------+\n|   3.5|2200156|\n|   4.5|1534824|\n|   2.5| 883398|\n|   1.0| 680732|\n|   4.0|5561926|\n|   0.5| 239125|\n|   3.0|4291193|\n|   2.0|1430997|\n|   1.5| 279252|\n|   5.0|2898660|\n+------+-------+\n\n"
                }
            ], 
            "source": "spark.read.format(\"parquet\")\\\n  .load(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/s3ba-4a57de59dd0705-90c0cef5c9f0/notebook/work/partitioned-files.parquet\").groupBy(\"rating\").count().show()"
        }, 
        {
            "source": "### Conclusion \n\nHere we are concluding that parquet is the best file format we could use with spark in a cost reducing and efficient way to reduce the capital expense and increase the operating efficiency while working with the Big data in cloud object storages(S3 , IBM COS)", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}